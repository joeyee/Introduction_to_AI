'''
Change the learning rate 'lr' and initial state 'x0', to see the convergence of the gradient descent method.
Check the function's final output, given different lr and x0.
Also can change the iterative times 'ep' to check the results.


Created by ZhouYi@Linghai_Dalian on 20250910.
'''

import math
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline

def f(x):
    #define the polynomial function f(x) = 3x^4 - 4x^3 - 12x^2 + 32
    f = 3*x**4 - 4*x**3 - 12*x**2 + 32
    return f

def df(x):
    #define the derivative of f(x) = 12x^3 - 12x^2 - 24x
    df = 12*x**3 - 12*x**2 - 24*x
    return df

xs = np.arange(-2,3,0.01)
fig, axs = plt.subplots(1,2)
axs[0].plot(xs,f(xs))
axs[0].set_xlabel('x')
axs[0].text(0, 50, r'f(x)', fontsize=20)
axs[0].grid()
axs[1].plot(xs,df(xs))
axs[1].set_xlabel('x')
axs[1].text(0, 100, r'$\frac{df(x)}{dx}$', fontsize=20)
axs[1].grid()


#（1）取不同初值时，梯度下降法的收敛情况。
#在x=0 和  x=2 处 存在梯度消失的问题。

#（2）取不同lr时，梯度下降法的收敛情况。
#x0=3
x0=0
lr = 0.6
#lr=0.01
ep = 7 #迭代次数

xs = np.arange(-2,3,0.01)
fig, axs = plt.subplots(1,2)
axs[0].plot(xs,f(xs))
axs[0].set_xlabel('x')
axs[0].text(0, 50, r'f(x)', fontsize=20)
axs[1].plot(xs,f(xs))
axs[1].text(-2, 60, f'x0={x0}, lr={lr:.4f}', fontsize=15)

for i in range(ep):
    x1 = x0 - lr*df(x0)
    x0 = x1
    #print(i, x0, df(x0), f(x0))
    axs[1].plot(x0,f(x0),'ro')
    axs[1].text(x0, f(x0), str(i), fontsize=10)
plt.show()
